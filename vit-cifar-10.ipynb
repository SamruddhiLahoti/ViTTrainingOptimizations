{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 6.684846,
     "end_time": "2023-11-18T03:29:12.205783",
     "exception": false,
     "start_time": "2023-11-18T03:29:05.520937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.122424,
     "end_time": "2023-11-18T03:29:12.335596",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.213172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006465,
     "end_time": "2023-11-18T03:29:12.348770",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.342305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VIT Implementation\n",
    "\n",
    "The vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n",
    "\n",
    "For the implementation, feel free to experiment different kinds of setup, as long as you use attention as the main computation unit and the ViT can be train to perform the image classification task present later.\n",
    "You can read about the ViT implement from other libary: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006585,
     "end_time": "2023-11-18T03:29:12.362047",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.355462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PatchEmbedding\n",
    "PatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.01635,
     "end_time": "2023-11-18T03:29:12.385043",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.368693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "      # TODO\n",
    "\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(self.in_channels, \n",
    "                            self.embed_dim, \n",
    "                            kernel_size=self.patch_size, \n",
    "                            stride=self.patch_size\n",
    "                           )\n",
    "        self.norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006301,
     "end_time": "2023-11-18T03:29:12.397587",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.391286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MultiHeadSelfAttention\n",
    "\n",
    "This class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.020356,
     "end_time": "2023-11-18T03:29:12.424622",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.404266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        # TODO\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(self.embed_dim, self.embed_dim * 3)\n",
    "        self.qk_norm = False\n",
    "        self.use_activation = False\n",
    "        self.activation = nn.ReLU() if self.use_activation else nn.Identity()\n",
    "        self.q_norm = nn.LayerNorm(self.head_dim) if self.qk_norm else nn.Identity()\n",
    "        self.k_norm = nn.LayerNorm(self.head_dim) if self.qk_norm else nn.Identity()\n",
    "        self.attn_dropout = nn.Dropout(0.1)\n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        batch_si, seq_len, emb_dim = x.shape\n",
    "        qkv = self.qkv(x).reshape(batch_si, seq_len, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attention = q @ k.transpose(-2, -1)\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        attention = self.attn_dropout(attention)\n",
    "\n",
    "        z = attention @ v\n",
    "        z = z.transpose(1, 2).reshape(batch_si, seq_len, emb_dim)\n",
    "        z = self.proj(z)\n",
    "#         z = self.proj_dropout(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006165,
     "end_time": "2023-11-18T03:29:12.437200",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.431035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TransformerBlock\n",
    "This class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\n",
    "You may also want to use layer normalization or other type of normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.015977,
     "end_time": "2023-11-18T03:29:12.459625",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.443648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
    "        # TODO\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.attention_norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_dim),\n",
    "                                  nn.GELU(),\n",
    "                                  nn.Dropout(dropout),\n",
    "                                  nn.Linear(mlp_dim, embed_dim)\n",
    "                                  # nn.Dropout(dropout)\n",
    "        )\n",
    "        self.mlp_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        res = x\n",
    "        x = self.attention_norm(x)\n",
    "        x = self.attention(x)\n",
    "        x = x + res # residual connection\n",
    "        res = x\n",
    "        x = self.mlp_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006164,
     "end_time": "2023-11-18T03:29:12.472241",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.466077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## VisionTransformer:\n",
    "This is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.019121,
     "end_time": "2023-11-18T03:29:12.497921",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.478800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
    "        # TODO\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.embed_len = self.patch_embed.num_patches + 1\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.embed_len, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for i in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.cls_head = nn.Sequential(nn.Linear(embed_dim, embed_dim//2),\n",
    "                                nn.GELU(),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(embed_dim // 2, num_classes),\n",
    "                                # nn.Dropout(dropout)     \n",
    "                                )                           \n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.patch_embed(x)\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        # x = self.dropout(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        # x = self.norm(x)\n",
    "        logits = self.cls_head(x[:, 0])\n",
    "\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.006289,
     "end_time": "2023-11-18T03:29:12.510729",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.504440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's train the ViT!\n",
    "\n",
    "We will train the vit to do the image classification with cifar100. Free free to change the optimizer and or add other tricks to improve the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.014057,
     "end_time": "2023-11-18T03:29:12.531267",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.517210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "image_size = 32\n",
    "patch_size = 4\n",
    "in_channels = 3\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "mlp_dim = 1024\n",
    "num_layers = 4\n",
    "num_classes = 10\n",
    "dropout = 0.1\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 13.309039,
     "end_time": "2023-11-18T03:29:25.846949",
     "exception": false,
     "start_time": "2023-11-18T03:29:12.537910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
    "# input_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\n",
    "# output = model(input_tensor)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 6.613343,
     "end_time": "2023-11-18T03:29:32.467052",
     "exception": false,
     "start_time": "2023-11-18T03:29:25.853709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.019645,
     "end_time": "2023-11-18T03:29:32.495798",
     "exception": false,
     "start_time": "2023-11-18T03:29:32.476153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
    "lr = 0.003\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 150\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(trainloader), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 8837.288569,
     "end_time": "2023-11-18T05:56:49.793622",
     "exception": false,
     "start_time": "2023-11-18T03:29:32.505053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 92/150 [54:41<34:28, 35.66s/it, Epoch=92, Train Accuracy=68.2, Training Loss=175, Validation Accuracy=70.3]  \n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 511933 (char 511932)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     43\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 44\u001b[0m     save_stats(step, loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m epoch_acc \u001b[38;5;241m=\u001b[39m acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainset)\n\u001b[1;32m     47\u001b[0m save_stats(epoch, epoch_acc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36msave_stats\u001b[0;34m(key, value, stat)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(stats_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 7\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 511933 (char 511932)"
     ]
    }
   ],
   "source": [
    "def save_stats(key, value, stat):\n",
    "    \n",
    "    stats_path = f\"stats/vit_baseline_cifar10_{stat}.txt\"\n",
    "    \n",
    "    try:\n",
    "        with open(stats_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        data = {}\n",
    "    \n",
    "    data[key] = value\n",
    "    \n",
    "    with open(stats_path, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "best_val_acc = 0\n",
    "epochs_no_improve = 0\n",
    "max_patience = 20\n",
    "early_stop = False\n",
    "step = -1\n",
    "\n",
    "pbar = tqdm(range(num_epochs))\n",
    "\n",
    "for epoch in pbar:\n",
    "\n",
    "    acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "        \n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "                \n",
    "        acc += (outputs.argmax(dim=1) == labels.to(device)).sum().item()\n",
    "        epoch_loss += loss.item()\n",
    "        save_stats(step, loss.item(), \"loss\")\n",
    "    \n",
    "    epoch_acc = acc / len(trainset)\n",
    "    save_stats(epoch, epoch_acc, \"acc\")\n",
    "\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "\n",
    "            outputs = model(images.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "            \n",
    "    val_acc = 100 * correct / len(testset)\n",
    "\n",
    "    pbar.set_postfix({\"Epoch\": epoch+1, \"Train Accuracy\": epoch_acc*100, \"Training Loss\": epoch_loss, \"Validation Accuracy\": val_acc})\n",
    "\n",
    "    # Save the best model\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        epochs_no_improve = 0\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer,\n",
    "#             'scheduler' : scheduler,\n",
    "        },  'saved_models/vit_baseline_cifar10.pth')\n",
    "\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epoch > 100 and epochs_no_improve >= max_patience:\n",
    "        print('Early stopping!')\n",
    "        early_stop = True\n",
    "        break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T05:56:49.859682Z",
     "iopub.status.busy": "2023-11-18T05:56:49.859321Z",
     "iopub.status.idle": "2023-11-18T05:56:49.864696Z",
     "shell.execute_reply": "2023-11-18T05:56:49.863755Z"
    },
    "papermill": {
     "duration": 0.040496,
     "end_time": "2023-11-18T05:56:49.866598",
     "exception": false,
     "start_time": "2023-11-18T05:56:49.826102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy: 87.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-18T05:56:51.063276Z",
     "iopub.status.busy": "2023-11-18T05:56:51.062954Z",
     "iopub.status.idle": "2023-11-18T05:56:51.171408Z",
     "shell.execute_reply": "2023-11-18T05:56:51.170538Z"
    },
    "papermill": {
     "duration": 0.144242,
     "end_time": "2023-11-18T05:56:51.173604",
     "exception": false,
     "start_time": "2023-11-18T05:56:51.029362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = torch.load('best_model.pth', map_location=device)\n",
    "model.load_state_dict(model_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040189,
     "end_time": "2023-11-18T05:56:55.967072",
     "exception": false,
     "start_time": "2023-11-18T05:56:55.926883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4019334,
     "sourceId": 6992928,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "idls",
   "language": "python",
   "name": "idls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8877.995773,
   "end_time": "2023-11-18T05:56:58.824741",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-18T03:29:00.828968",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
